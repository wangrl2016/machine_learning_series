<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="../laptop_coding_48.png" type="image/png">
    <title>注意力机制</title>
    <!-- 引入 Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        .sidebar {
            position: sticky;
            top: 0;
            height: 100vh;
            overflow-y: auto;
            background-color: #f8f9fa;
            color: #000;
            border-right: 1px solid #ddd;
        }

        .custom-title {
            font-size: 2.5rem;
            font-weight: 700;
            color: #4caf50;
            text-align: center;
            margin-bottom: 20px;
            border-bottom: 3px solid #4caf50; /* 标题下边框 */
        }

        .title-subtext {
            font-size: 1.2rem;
            color: #6c757d; /* Bootstrap 灰色 */
            text-align: center;
        }
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background: #f4f4f9;
            color: #333;
        }
        .content {
            padding: 20px;
            background: #fff;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        /* 移动端样式（只在屏幕宽度 < 1024 px 时生效 */
        @media screen and (max-width: 1024px) {
            .sidebar, .right-sidebar {
                display: none; /* 隐藏侧边栏 */
            }
            .content {
                margin: 0;
                padding: 10px;
                width: 100%;
            }
        }
        .content h2 {
            width: 100%;
            font-size: 3rem;
            line-height: 1.6;
            color: #4caf50;
            text-align: center;
            margin-bottom: 40px;
            border-bottom: 3px solid #4caf50;
            display: inline-block;
            padding-bottom: 10px;
            font-weight: bold;
            text-shadow: 1px 1px 5px rgba(0, 0, 0, 0.1);
        }
        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 16px;
            line-height: 1.5;
        }
        .added {
            color: #10e6e2;
            display: inline-block;
            width: 100%;
        }
        code {
            font-family: "Courier New", Courier, monospace;
        }
        .content h3 {
            margin-top: 60px;
            color: #8e44ad;
        }
        .content h4 {
            margin-top: 40px;
            color: #8e44ad;
        }
        .content p {
            font-size: 1.1em;
            margin: 20px 0;
        }
        .highlight-math {
            background-color: #FFCC99;
            padding: 5px;
            border-radius: 5px;
        }
        .highlight-link {
            color: #f39c12;
            font-weight: bold;
        }
        .highlight {
            background-color: #f1c40f;
            font-weight: bold;
        }
        pre {
            background: #E3F2FD;
            color: #212121;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            font-size: 0.9em;
        }
        ul {
            list-style: none;
            padding-left: 20px;
            margin: 10px 0;
        }
        .with-bullets {
            list-style: disc;
            padding-left: 10px;
            margin: 10px 0 10px 20px;
        }
        ul li {
            margin: 4px 0;
            color: #34495e;;
        }
        li .title {
            font-weight: bold;
            color: #2c3e50;
            font-size: 18px;
            margin-bottom: 6px;
        }
        li p {
            color: #34495e;
            font-size: 16px;
        }
        .right-sidebar {
            position: sticky;
            top: 0;
            height: 100vh;
            overflow-y: auto;
            padding: 20px;
            background-color: #f8f9fa;
            border-left: 1px solid #ddd;
        }
        .right-sidebar a {
            color: #343a40;
            text-decoration: none;
        }
        .right-sidebar a:hover {
            text-decoration: underline;
        }
        .right-sidebar .active {
            font-weight: bold;
            color: #007bff;
        }
        #dynamic-sidebar ul {
            list-style-type: none;
            padding-left: 0;
        }
        #dynamic-sidebar ul ul {
            padding-left: 20px;
        }
        a {
            text-decoration: underline;
            text-underline-offset: 4px;
            color: #3498db;
        }
        a:hover {
            color: #2c3e50;
        }
        img {
            display: block;
            margin: 0 auto;
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15), 0 -2px 6px rgba(0, 0, 0, 0.1);;
        }
        .comment {
            color: #6c757d;
            font-size: 0.9em;
            font-style: italic;
            margin: 20px 0;
            border-left: 3px solid #d6d8db;
            padding-left: 10px;
        }
        table {
            width: 50%;
            border-collapse: collapse;
            margin: 20px auto;
            font-family: Arial, sans-serif;
        }
        th, td {
            border: 1px solid #ccc;
            padding: 10px;
            text-align: center;
        }
        th {
            background-color: #f4f4f4;
        }
        .positive {
            color: green;
            font-weight: bold;
        }
        .negative {
            color: red;
            font-weight: bold;
        }
        .scroll-container {
            width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
            -ms-overflow-style: none;
            scrollbar-width: none;
        }
    </style>
</head>
<body>
    <div class="container-fluid">
        <div class="row">
        <!-- 左侧小节列表 -->
        <nav class="col-md-2 sidebar">

        </nav>

        <!-- 中间内容：显示具体某一个小节 -->
        <main class="col-md-8 content">
            <h2 class="custom-title">9.1 注意力机制</h2>
            <link rel="icon" href="../laptop_coding_48.png" type="image/png">
            <p class="title-subtext">注意力机制 (Attention Mechanism) ！</p>
            <p style="color: #a0a0a0;">创建日期: 2025-01-18</p>

            <p><strong>注意力机制 (Attention Mechanism)</strong> 是一种机器学习技术，它指导深度学习模型优先考虑（或关注）输入数据中最相关的部分。注意力机制的创新促成了 Transformer 架构的诞生，该架构催生了 <em>现代大语言模型 (Large Language Model, LLM)</em> ，为 ChatGPT 等热门应用程序提供支持。</p>

            <h3>9.1.1 历史发展</h3>

            <p>顾名思义，注意力机制的灵感来源于人类（和其它动物）选择性地关注突出细节并忽略当前不太重要的细节的能力。获取所有信息但只关注最相关的信息有助于确保不会丢失任何有意义的细节，同时还能有效利用有限的内存和时间。</p>

            <p>在心理学中，注意力（Attention）是指人类大脑对特定信息的优先选择和处理的能力，是认知心理学的核心研究领域之一。注意力不仅影响我们的感知、记忆和决策，还涉及多种行为任务的完成。</p>

            <p>从数学上讲，注意力机制计算 <strong>注意力权重 (Attention Weight) </strong>，该权重反映了输入序列的每个部分对当前任务的相对重要性。然后，它根据输入序列每个部分的重要性，应用这些注意力权重来增加（或减少）输入序列每个部分的影响。注意力模型（即采用注意力机制的人工智能模型）通过对大量示例进行监督学习或自我监督学习来训练，以分配准确的注意力权重。</p>
            <p>注意力机制最初由 Bahdanau 等人于 2014 年提出（论文 <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a>），作为一种技术来解决当时用于机器翻译的最先进的循环神经网络 (RNN) 模型的缺点。后续研究将注意力机制整合到用于图像字幕和视觉问答等任务的卷积神经网络 (CNN) 中。</p>
            <p>2017 年，开创性的论文 <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> 引入了 Transformer 模型，该模型完全摒弃了循环和卷积，只采用注意力层和标准前馈层。从那时起，Transformer 架构就成为了推动生成式 AI 时代发展的尖端模型的支柱。</p>
            <p>虽然注意力机制主要与用于自然语言处理 (NLP) 任务（例如摘要、问答、文本生成和情感分析）的 LLM 相关，但基于注意力的模型也广泛应用于其他领域，先进的图像生成的扩散模型通常包含注意力机制。</p>
            <p>了解了注意力机制的历史，接下来通过代码理解注意力在计算机中是如何实现！</p>

            <h3>9.1.2 注意力提示</h3>
            <p>注意力是如何应用于视觉世界中的呢？这要从当今十分普及的双组件框架说起，这个框架的出现可以追溯到 19 世纪 90 年代的美国信息学之父威廉·詹姆斯，他是当时最具影响力的心理学家和哲学家之一，在其著作《心理学原理》中，将注意分为主动注意和被动注意：</p>
            <ul>
                <li>
                    <div class="title">1. 被动注意 (Passive Attention) </div>
                    <p>定义：被动注意是指我们的注意力不由自主地被外界刺激所吸引，而将注意力集中在它上面。这种注意力通常是无意识的，不需要付出努力。</p>
                    <ol>
                        <li>听到巨大的声响而转头去看；</li>
                        <li>被色彩鲜艳的广告牌所吸引；</li>
                        <li>在人群中突然听到自己的名字；</li>
                        <li>看到移动的物体；</li>
                        <li>闻到香味。</li>
                    </ol>
                    <p>越响、越亮、越鲜艳的刺激越容易引起被动注意。</p>
                </li>
                <li>
                    <div class="title">2. 主动注意 (Active Attention) </div>
                    <p>定义：主动注意是指有意识地、主动地将注意力集中在某个事物或任务上，这种注意是有目的的，需要付出努力和意志力。</p>
                    <ol>
                        <li>在嘈杂的环境中专心阅读一本书；</li>
                        <li>在会议上集中精力听取发言人讲话；</li>
                        <li>为了完成工作而集中精力处理文件；</li>
                        <li>学习新的技能；</li>
                        <li>解决复杂的问题。</li>
                    </ol>
                    <p>倾向于关注自己感兴趣或与目标相关的事物。</p>
                </li>
            </ul>
            <p>在日常生活中，主动注意和被动注意并不是完全独立的，它们经常相互作用。例如，我们可能主动选择去听一场音乐会（主动注意），但在音乐会中，我们可能会被美妙的旋律或歌手的表演所吸引（被动注意）。</p>

            <h3>9.1.3 查询、键和值</h3>
            <p>在计算机科学的注意力机制中，<strong>查询 (Query)</strong> 、<strong>键 (Key)</strong> 和 <strong>值 (value)</strong> 的核心思想是模仿人类的注意力机制来动态选择重要信息。具体含义如下：</p>
            <ul>
                <li>
                    <div class="title">1. 查询 (Query) </div>
                    <p>表示我们想要寻找什么或关注什么，类似于心理学中主动注意的目标，是由任务需求驱动的。</p>
                </li>
                <li>
                    <div class="title">2. 键 (Key) </div>
                    <p>表示数据中的潜在目标，是系统要扫描的对象特征，类似于心理学中被动注意机制的外部刺激。</p>
                </li>
                <li>
                    <div class="title">3. 值 (Value)</div>
                    <p>表示与某个键相关联的内容或信息，类似于心理学中关注后的实际信息提取。</p>
                </li>
            </ul>
            <p>主动注意对应的是 Query 的作用——明确地表示“我要寻找什么”，例如在阅读文章时，主动注意会将注意力集中在与目标（Query）相关的关键词（Key）上，提取对应的值（Value）。</p>
            <p>被动注意对应的是 Key 的显著性作用——环境中的某些刺激自动吸引注意。被动注意的触发依赖于键值对的匹配。当一个人分心时，环境中的突发噪音（Key）触发了无意识的注意力转移，导致人关注这个刺激，并提取相应的信息（Value）。</p>

            <p>主动注意与被动注意解释了人类的注意力方式，下面来看看如何通过这两个注意力，用神经网络来设计注意力机制的框架。</p>
            <p>首先，考虑一个相对简单情况，即只使用被动注意。要想将选择偏向于感官输入，则可以简单地使用参数化的全连接层，甚至是非参数化的最大汇聚层或平均汇聚层。</p>
            <p>因此，是否包含主动注意将注意力机制与全连接层或汇聚层区别开来。在注意力机制的背景下，主动注意被称为查询。给定任何查询，注意力机制通过 <strong>注意力汇聚 (Attention Pooling)</strong> 将选择引导至感官输入中，它们被称为值 (Value) 。</p>
            <p>更通俗的解释是，每个值 (Value) 都与一个键 (Key) 匹配，这可以理解成感官的被动注意，如下图所示，可以通过设计注意力汇聚的方式，便于给定的查询与键进行匹配，这将引导得到最匹配的值（感官输入）：</p>
            <a href="https://drive.google.com/file/d/1wFzP8od_swcNgA-cWwpBCh7Lqdig3MP-/view?usp=sharing">
                <img src="res/attention_pooling.svg" alt="注意力汇聚" width="600" >
            </a>
            <h3>9.1.4 注意力汇聚</h3>
            <p>查询（主动注意）和键（被动注意）之间的交互形成了注意力汇聚，注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出。接下来介绍注意力汇聚的更多细节，以便从宏观上了解注意力机制在实践中的运作方式。</p>
            <p>具体来说，1964 年提出的 <strong>Nadaraya-Watson 核回归模型</strong> 是一个简单但完整的例子，可以用于演示具有注意力机制的机器学习。</p>

            <h4>9.1.4.1 生成数据集</h4>
            <p>简单起见，考虑下面的回归问题：给定成对的（输入-输出）数据集 \(\lbrace(x_1, y_1), ... , (x_n, y_n)\rbrace\) ，如何学习 \(f\) 来预测任意新的输入 \(x\) 的输出 \(\hat{y} = f(x)\) ？</p>
            <p>根据下面的非线性函数生成一个人工数据集： </p>
            <p class="highlight-math" style="text-align: center;">\(y_i = 2sin(x_i) + x_i^{0.8} + ξ\)</p>
            <p>其中 \(ξ\) 为加入的噪声项，服从均值为 0 和标准差为 0.5 的正态分布。在这里生成了 50 个训练样本。为了更好地可视化，需要对训练样本进行排序，代码在 <a href="https://github.com/artinte/machine-learning-series/blob/develop/deep_learning/09_transformer/1_attention/random_point_dataset.py">random_point_dataset.py</a> 文件里：</p>
            <pre><code>def func(x):
    return 2 * numpy.sin(x) + x**0.8

rng = numpy.random.default_rng(0)
n_train = 50
x_train = numpy.sort(rng.random(n_train) * 5)
y_train = func(x_train) + rng.normal(0.0, 0.5, (n_train,))
x = numpy.arange(0, 5, 0.05)
y_truth = func(x)</code></pre>
            <img src="res/random_point_dataset.png" alt="随机点数据集" width="600">

            <h4>9.1.4.2 平均汇聚</h4>
            <p>先使用最简单的估计器来解决回归问题，基于平均汇聚来计算所有训练样本输出的平均值：</p>
            <p class="highlight-math" style="text-align: center;">\(f(x) = \frac{1}{n}\sum_{i=1}^{n} y_i\)</p>
            <p>代码实现在 <a href="https://github.com/artinte/machine-learning-series/blob/develop/deep_learning/09_transformer/1_attention/average_pooling.py">average_pooling.py</a> 文件里，学习得到的预测函数使用 Pred 进行标记：</p>
            <pre><code>y_pred = y_train.mean().repeat(len(x))</code></pre>
            <p>如下图所示，这个估计器确实不够聪明，真实函数 (Truth) 和预测函数 (Pred) 相差很大：</p>
            <img src="res/average_pooling.png" alt="平均汇聚" width="600">

            <h4>9.1.4.2 无参注意汇聚</h4>
            <p>显然，平均汇聚忽略了输入 \(x_i\) 。于是 Nadaraya 和 Watson 提出了一个更好的想法，根据输入的位置对输出 \(y_i\) 进行加权：</p>
            <p class="highlight-math" style="text-align: center;">\(f(x) = \sum_{i=1}^{n} \frac{K(x - x_i)}{\sum_{j=1}^{n} K(x - x_j)} y_i\)</p>
            <p>其中 \(K\) 是 核 ，上述公式被称为 Nadaraya-Watson 核回归。这里不会深入讨论核函数细节，但受此启发，我们可以得到一个更加通用的注意力汇聚公式：</p>
            <p class="highlight-math" style="text-align: center;">\(f(x) = \sum_{i=1}^{n} \alpha(x, x_i)y_i\)</p>
            <p>其中 \(x\) 是查询，\((x_i, y_i)\) 是键值对，将查询 \(x\) 和键 \(x_i\) 之间的关系建模为注意力权重 \(\alpha(x, x_i)\) ，这个权重将被分配给每一个对应值 \(y_i\) 。对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为 1 。</p>
            <p>为了更好地理解注意力汇聚，下面考虑一个 <em>高斯核 (Gaussian Kernel)</em> ，其定义为：</p>
            <p class="highlight-math" style="text-align: center;">\(K(\mu) = \frac{1}{\sqrt{2\pi}} exp(-\frac{{{\mu}^2}}{2})\)</p>

            <p>将高斯核代入注意力汇聚公式可以得到：</p>
            <div class="scroll-container highlight-math" style="text-align: center;">
                \(f(x) = \sum_{i=1}^{n} \alpha(x, x_i)y_i = \sum_{i=1}^{n}softmax(-\frac{1}{2}(x - x_i)^2)y_i\)
            </div>
            <p>上面公式表明，如果一个键 \(x_i\) 越是接近给定的查询 \(x\) ，那么分配给这个键对应值 \(y_i\) 的注意力权重就会越大，也就是“获得了更多的注意力”，代码在 <a href="https://github.com/artinte/machine-learning-series/blob/develop/deep_learning/09_transformer/1_attention/nonparam_pooling.py">nonparam_pooling.py</a> 文件里：</p>
            <pre><code># Each row contains the same input (query).
x_pred_repeat = x.repeat(n_train).reshape((-1, n_train))
# shape: (n_pred, n_train)
attention_weights = torch.nn.functional.softmax(-(torch.tensor(x_pred_repeat - x_train))**2 / 2, dim=1)
print('Attention weigths shape:', attention_weights.shape)
y_hat = torch.matmul(attention_weights, torch.tensor(y_train))</code></pre>
            <p>注意力权重的维度由预测的次数和训练样本的个数确定：</p>
            <pre><samp>Attention weigths shape: torch.Size([100, 50])</samp></pre>
            <p>Nadaray-Waston 核回归是一个无参模型，接下来我们将基于这个模型来绘制预测结果。从绘制的结果会发现新的模型预测是平滑的，比平均汇聚的预测更接近真实。</p>
            <img src="res/nonparam_pooling.png" alt="无参汇聚" width="600">
            <p>文件 <a href="https://github.com/artinte/machine-learning-series/blob/develop/deep_learning/09_transformer/1_attention/show_heatmap.py">show_heatmap.py</a> 提供了一个绘制热力图的函数，能够可视化注意力权重：</p>
            <pre><code>def show_heatmap(matrices, x_label = '', y_label = '', cmap='Reds'):
    num_rows, num_cols = matrices.shape[0], matrices.shape[1]
    fig, axes = pyplot.subplots(num_rows, num_cols,
                                sharex=True, sharey=True, squeeze=False)
    # shape os axes: (1 x 1)
    for row_idx, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
        for col_idx, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
            pcm = ax.imshow(matrix, cmap=cmap)
            if row_idx == num_rows - 1:
                ax.set_xlabel(x_label)
            if col_idx == 0:
                ax.set_ylabel(y_label)
    fig.colorbar(pcm, ax=axes, shrink=0.6)
    fig.subplots_adjust(left=0.1, right=0.96, top=0.96, bottom=0.1, wspace=0.2, hspace=0.2)
    pyplot.show()</code></pre>
            <img src="res/nonparam_heatmap.png" alt="无参热力图" width="600">

            <h4>9.1.4.3 有参注意汇聚</h4>
            <p>无参数的 Nadaraya-Watson 核回归具有一致性的优点：如果有足够的数据，此模型会收敛到最优结果。尽管如此，我们还是可以轻松地将可学习的参数集成到注意力汇聚中。</p>
            <p>与无参注意汇聚公式略有不同，在将查询 \(x\) 和键 \(x_i\) 之间的距离乘以可学习参数 \(w\)：</p>
            <div class="scroll-container highlight-math"  style="text-align: center;">
                \(f(x) = \sum_{i=1}^{n} \alpha(x, x_i)y_i = \sum_{i=1}^{n}softmax(-\frac{1}{2}((x - x_i)w)^2)y_i\)
            </div>
            <p>为了更有效地计算小批量数据的注意力，我们可以利用批量矩阵乘法。假设第一个小批量数据包括 \(n\) 个矩阵 \(X_1, ... , X_n\) ，矩阵的形状为 \(a \times b\) ，第二个小批量包含 \(n\) 个矩阵 \(Y_1, ... , Yn\) ，矩阵的形状为 \(b \times c\) 。</p>
            <p>它们的批量矩阵乘法得到 \(n\) 个矩阵 \(X_{1}Y_1, ... , X_{n}Y_n\) ，形状为 \(a \times c\) 。因此，假定两个张量的形状分别为 \((n, a, b)\) 和 \((n, b, c)\) ，它们的批量矩阵乘法输出的形状为 \((n, a, c)\) ，具体计算过程在 <a href="https://github.com/artinte/machine-learning-series/blob/develop/deep_learning/09_transformer/1_attention/batch_array_dot.py">batch_array_dot.py</a> 文件里：</p>
            <pre><code>x = torch.ones((2, 1, 4))
y = torch.ones((2, 4, 6))
print(torch.bmm(x, y).shape)</code></pre>
            <pre><samp>torch.Size([2, 1, 6])</samp></pre>
            <p>在注意力机制的背景中，我们可以使用小批量矩阵乘法来计算小批量数据中的加权平均值：</p>
            <pre><code>weights = (torch.ones((2, 10)) * 0.1).unsqueeze(1)
values = torch.arange(20.0).reshape((2, 10)).unsqueeze(-1)
print('Weights shape:', weights.shape)
print('Values shape:', values.shape)
result = torch.bmm(weights, values)
print('Result shape:', result.shape)</code></pre>
            <pre><samp>Weights shape: torch.Size([2, 1, 10])
Values shape: torch.Size([2, 10, 1])
Result shape: torch.Size([2, 1, 1])</samp></pre>

            <p>基于带参数的注意力汇聚，使用小批量矩阵乘法，定义带参数的 Nadaraya-Watson 核回归版本，代码在 <a href="https://github.com/artinte/machine-learning-series/blob/develop/deep_learning/09_transformer/1_attention/param_pooling.py">param_pooling.py</a> 文件里：</p>
            <pre><code>class NWKernelRegression(torch.nn.Module):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.w = torch.nn.Parameter(torch.tensor([0.1]), requires_grad=True)

    def forward(self, queries, keys, values):
        queries = queries.repeat_interleave(keys.shape[1]).reshape((-1, keys.shape[1]))
        self.attention_weights = torch.nn.functional.softmax(
            -((queries - keys) * self.w) ** 2 / 2, dim=1)
        return torch.bmm(self.attention_weights.unsqueeze(1),
                            values.unsqueeze(-1)).reshape(-1)</code></pre>
            <p>训练带参数的注意力汇聚模型时，使用平方损失和随机梯度下降：</p>
            <pre><code>net = NWKernelRegression()
loss = torch.nn.MSELoss(reduction='none')
trainer = torch.optim.SGD(net.parameters(), lr=0.5)

for epoch in range(5):
    trainer.zero_grad()
    l = loss(net(x_train, keys, values), y_train)
    l.sum().backward()
    trainer.step()
    print(f'Epoch {epoch + 1}, loss {float(l.sum().item()):.6f}')</code></pre>
            <p>如下所示，训练完带参数的注意力汇聚模型后可以发现：在尝试拟合带噪声的训练数据时，预测结果绘制的线不如之前非参数模型的平滑：</p>
            <img src="res/param_pooling.png" alt="有参注意汇聚" width="600">
            <p>为什么新的模型更不平滑呢？下面看一下输出结果的绘制图：与非参数的注意力汇聚模型相比，带参数的模型加入可学习的参数后，曲线在注意力权重较大的区域变得更不平滑：</p>
            <img src="res/param_heatmap.png" alt="有参热力图" width="600">
        </main>

            <!-- 右侧小标题导航 -->
            <aside class="col-md-2 right-sidebar">
                <h5><a href="index.html#section9">Transformer 架构</a></h5>
                <ul id="dynamic-sidebar" class="nav flex-column">
                    <!-- 动态内容 -->
                </ul>
            </aside>
        </div>
    </div>

        <!-- 引入 Bootstrap JS -->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <script>
            // 获取所有的 h3 和 h4 元素
            const sidebar = document.getElementById('dynamic-sidebar');
            const elements = Array.from(document.querySelectorAll('h3, h4')); // 获取所有 h3 和 h4 元素

            // 动态生成目录项
            elements.forEach(element => {
                // 如果没有 id，则为该元素生成一个 id
                if (!element.id) {
                    element.id = `${element.tagName.toLowerCase()}-${Math.random().toString(36).substr(2, 9)}`;
                }

                const link = document.createElement('a');
                link.href = `#${element.id}`;
                link.textContent = element.textContent;

                const listItem = document.createElement('li');
                // 如果是 h4 元素，添加缩进
                if (element.tagName.toLowerCase() === 'h4') {
                    listItem.style.marginLeft = '20px'; // 控制缩进
                }
                listItem.appendChild(link);

                sidebar.appendChild(listItem);
            });

            // 高亮当前目录项
            function highlightCurrentLink(targetId) {
                const links = sidebar.querySelectorAll('a');
                links.forEach(link => {
                    if (link.getAttribute('href') === `#${targetId}`) {
                        link.classList.add('highlight-link');
                    } else {
                        link.classList.remove('highlight-link');
                    }
                });
            }

            // 页面滚动时更新高亮
            window.addEventListener('scroll', () => {
                let found = false;
                elements.forEach(el => {
                    const rect = el.getBoundingClientRect();
                    if (rect.top <= window.innerHeight / 2 && rect.bottom >= 0) {
                        if (!found) {
                            highlightCurrentLink(el.id);
                            found = true;
                        }
                    }
                });
            });

            // 为每个目录链接添加点击事件
            const links = sidebar.querySelectorAll('a');
            links.forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = link.getAttribute('href').substring(1);
                    const targetElement = document.getElementById(targetId);
                    if (targetElement) {
                        targetElement.scrollIntoView({ behavior: 'smooth' });
                        highlightCurrentLink(targetId);
                    }
                });
            });
        </script>
</body>
</html>
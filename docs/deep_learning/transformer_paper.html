<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer 论文</title>
    <!-- 引入 Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        .sidebar {
            position: sticky;
            top: 0;
            height: 100vh;
            overflow-y: auto;
            background-color: #f8f9fa;
            color: #000;
            border-right: 1px solid #ddd;
        }

        .custom-title {
            font-size: 2.5rem;
            font-weight: 700;
            color: #4caf50;
            text-align: center;
            margin-bottom: 20px;
            border-bottom: 3px solid #4caf50; /* 标题下边框 */
        }

        .title-subtext {
            font-size: 1.2rem;
            color: #6c757d; /* Bootstrap 灰色 */
            text-align: center;
        }
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background: #f4f4f9;
            color: #333;
        }
        .content {
            padding: 20px;
            background: #fff;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        /* 移动端样式（只在屏幕宽度 < 1024 px 时生效 */
        @media screen and (max-width: 1024px) {
            .sidebar, .right-sidebar {
                display: none; /* 隐藏侧边栏 */
            }
            .content {
                margin: 0;
                padding: 10px;
                width: 100%;
            }
        }
        .content h2 {
            width: 100%;
            font-size: 3rem;
            line-height: 1.6;
            color: #4caf50;
            text-align: center;
            margin-bottom: 40px;
            border-bottom: 3px solid #4caf50;
            display: inline-block;
            padding-bottom: 10px;
            font-weight: bold;
            text-shadow: 1px 1px 5px rgba(0, 0, 0, 0.1);
        }
        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 16px;
            line-height: 1.5;
        }
        .added {
            color: #10e6e2;
            display: inline-block;
            width: 100%;
        }
        code {
            font-family: "Courier New", Courier, monospace;
        }
        .content h3 {
            margin-top: 60px;
            color: #8e44ad;
        }
        .content h4 {
            margin-top: 40px;
            color: #8e44ad;
        }
        .content p {
            font-size: 1.1em;
            margin: 20px 0;
        }
        .highlight-math {
            background-color: #FFCC99;
            padding: 5px;
            border-radius: 5px;
        }
        .highlight-link {
            color: #f39c12;
            font-weight: bold;
        }
        .highlight {
            background-color: #f1c40f;
            font-weight: bold;
        }
        pre {
            background: #263238;
            color: #c3e88d;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            font-size: 0.9em;
        }
        ul {
            list-style: none;
            padding-left: 20px;
            margin: 10px 0;
        }
        .with-bullets {
            list-style: disc;
            padding-left: 10px;
            margin: 10px 0 10px 20px;
        }
        ul li {
            margin: 4px 0;
            color: #34495e;;
        }
        li .title {
            font-weight: bold;
            color: #2c3e50;
            font-size: 18px;
            margin-bottom: 6px;
        }
        li p {
            color: #34495e;
            font-size: 16px;
        }
        .right-sidebar {
            position: sticky;
            top: 0;
            height: 100vh;
            overflow-y: auto;
            padding: 20px;
            background-color: #f8f9fa;
            border-left: 1px solid #ddd;
        }
        .right-sidebar a {
            color: #343a40;
            text-decoration: none;
        }
        .right-sidebar a:hover {
            text-decoration: underline;
        }
        .right-sidebar .active {
            font-weight: bold;
            color: #007bff;
        }
        #dynamic-sidebar ul {
            list-style-type: none;
            padding-left: 0;
        }
        #dynamic-sidebar ul ul {
            padding-left: 20px;
        }
        a {
            text-decoration: underline;
            text-underline-offset: 4px;
            color: #3498db;
        }
        a:hover {
            color: #2c3e50;
        }
        img {
            display: block;
            margin: 0 auto;
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15), 0 -2px 6px rgba(0, 0, 0, 0.1);;
        }
        .comment {
            color: #6c757d;
            font-size: 0.9em;
            font-style: italic;
            margin: 20px 0;
            border-left: 3px solid #d6d8db;
            padding-left: 10px;
        }
        table {
            width: 50%;
            border-collapse: collapse;
            margin: 20px auto;
            font-family: Arial, sans-serif;
        }
        th, td {
            border: 1px solid #ccc;
            padding: 10px;
            text-align: center;
        }
        th {
            background-color: #f4f4f4;
        }
        .positive {
            color: green;
            font-weight: bold;
        }
        .negative {
            color: red;
            font-weight: bold;
        }
        .scroll-container {
            width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
            -ms-overflow-style: none;
            scrollbar-width: none;
        }
        .paper-comment {
            background-color: #d3d3d3;
            padding: 10px;
            border-radius: 5px;
            font-size: 16px;
            border: 1px solid #a9a9a9;
        }
    </style>
</head>
<body>
    <div class="container-fluid">
        <div class="row">
        <!-- 左侧小节列表 -->
        <nav class="col-md-2 sidebar">

        </nav>

        <!-- 中间内容：显示具体某一个小节 -->
        <main class="col-md-8 content">
            <h2 class="custom-title">9.4 Transformer 论文</h2>
            <p class="title-subtext"><strong style="color: red;">注意力 (Attention)</strong> ，你只需要它！</p>
            <p style="color: #a0a0a0;">创建日期: 2025-01-23</p>
            <p>论文作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin</p>
            <p>主要的 <strong>序列传导模型 (Sequence Transduction Model)</strong> 基于复杂的循环或者卷积神经网络，它们包含一个 <strong>编码器 (Encoder)</strong> 和一个 <strong>解码器 (Decoder)</strong>。性能最佳的模型还通过 <strong>注意力机制 (Attention Mechanism)</strong> 连接编码器和解码器。我们提出一种新的简单的网络架构 <strong style="color: red;">Transformer</strong> ，它只基于注意力机制，完全省去了循环和卷积操作。</p>
            <p>在两个 <em>机器翻译 (Machine Translation)</em> 任务上的实验表明，模型质量优越，并行性更高，并且训练时间明显减少。在 WMT 2014 英语翻译成德语的任务中，我们的模型获得 28.4 BLEU 分数，比现有最佳结果，包括 <em>集成 (Ensemble)</em> 模型，至少提高 2 BLEU 分数。在 WMT 2014 英语翻译成法语的任务中，我们的模型在 8 块 GPU 上训练了 3.5 天，达到了 41.8 BLEU 分数，创新了单模型最好成绩，其训练成本仅为其它最佳模型的一小部分。</p>
            <p class="paper-comment">
                WMT 是 <em>Workshop on Machine Translation</em> 的缩写，WMT 14 指的是 2014 年的机器翻译研讨会，它提供了 <em>英-法</em> 和 <em>英-德</em> 语言数据集。
            </p>
            <p class="paper-comment">
                BLEU 是一个自动评价机器翻译的方法，来自论文 <a href="https://aclanthology.org/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a> ，它通过比较机器翻译结果与人类参考翻译的 N-gram 重叠度来衡量翻译质量，分数越高表示翻译质量越好。
            </p>
            <p>我们通过将 Transformer 成功应用于英语翻译（无论是大量还是有限训练数据的情况下），证明了 Transformer 能很好地泛化到其它任务。</p>

            <p class="comment">8 位作者对论文有同等贡献，名字顺序是随机的，他们每个人所作的工作如下：</p>
            <ul class="with-bullets">
                <li>
                    <p>Jakob 建议用自注意力取代 RNN，并开始努力评估这一想法。</p>
                </li>
                <li>
                    <p>Ashish 和 Illia 设计并实现了第一个 Transformer 模型，并在该工作的方方面面都发挥了至关重要的作用。</p>
                </li>
                <li>
                    <p>Noam 提出了缩放点积注意力机制、多头注意力机制和无参位置表示，也参与了许多细节工作。</p>
                </li>
                <li>
                    <p>Niki 在我们的代码仓和 <a href="https://github.com/google/trax">tensor2tensor</a> （tensor2tensor 已经使用 trax 代替）中设计、实现、调整和评估了无数的模型变体。</p>
                </li>
                <li>
                    <p>Llion 也尝试了模型变体，负责最初的代码库设计以及高效的推理和可视化。</p>
                </li>
                <li>
                    <p>Lukasz 和 Aidan 花了无数个漫长的日子设计并实现了 tensor2tensor 的各个部分，替换了我们之前的代码库，大大改善了结果，并加快了研究。</p>
                </li>
            </ul>
            <h3>9.4.1 介绍</h3>
            <p>循环神经网络，特别是长短期记忆 (LSTM) 和门控循环 (GRU) 神经网络，已经被认为是解决 <em>序列建模 (Sequence Modeling)</em>和传导问题（比如语言建模和机器翻译）的最先进方法。许多研究工作在持续推动循环语言模型和 <em>编码器-解码器 (Encoder-Decoder) 架构的边界。</em></p>
            <p class="paper-comment">LSTM 来自于论文 <a href="https://deeplearning.cs.cmu.edu/S23/document/readings/LSTM.pdf">Long Short-Term Memory</a> ，GRU 来自于论文 <a href="https://arxiv.org/abs/1412.3555">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a> 。</p>
            <p class="comment">语言建模、机器翻译和编码器-解码器相关研究论文：</p>
            <ol>
                <li><p><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1609.08144">Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1602.02410">Exploring the Limits of Language Modeling</a></p></li>
            </ol>
            <p>循环模型通常根据输入和输出序列的符号位置进行计算。通过位置与计算时间步骤对齐，它们会生成一系列的隐状态 \(h_t\) ，它是之前隐状态 \(h_{t-1}\) 和位置 \(t\) 输入的函数。这种固有的顺序性阻碍了训练样本的并行化，这在较长的序列中尤其重要，因为内存限制了样本之间的批处理。</p>
            <p>最近的研究通过 <em>分解技巧 (Factoization Trick)</em> 和 <em>条件计算 (Conditional Computation)</em> ，显著提高了计算效率，同时后者也能提高模型性能。然而，顺序计算的基本约束仍然存在。</p>
            <p class="paper-comment">分解技巧来自于论文 <a href="https://arxiv.org/abs/1703.10722">Factorization tricks for LSTM networks</a> ，条件计算来自于论文 <a href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a> 。</p>
            <p>注意力机制已经称为序列建模和传导模型不可或缺的一部分，它允许对依赖关系进行建模，而无需考虑它们在输入或输出序列中的距离。然而，除了少数情况外，这种注意力机制都是与循环网络结合使用的。</p>
            <p class="paper-comment">论文 <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> 和 <a href="https://arxiv.org/abs/1702.00887">Structured Attention Networks</a> 使用注意力机制，论文 <a href="https://arxiv.org/abs/1606.01933">A Decomposable Attention Model for Natural Language Inference</a> 使用注意力将问题分解成可以解决的子问题，实现并行化。</p>
            <p>在这项工作中，我们提出了 Transformer ，这是一种避免循环、完全依靠注意力机制来绘制输入和输出之间的全局依赖关系的架构。Transformer 显著提高了并行化水平，在 8 个 P100 GPU 上训练 12 小时后，就可以达到最新的翻译质量水平。</p>

            <h3>9.4.2 背景</h3>

            <h3>9.4.3 模型架构</h3>

            <h4>9.4.3.1 编码器和解码器栈</h4>
            
            <h4>9.4.3.2 注意力</h4>

            <p>缩放点积注意力</p>
            <p>多头注意力</p>
            <p>注意力在模型的应用</p>

            <h4>9.4.3.3 位置前馈网络</h4>
            <h4>9.4.3.4 嵌入和 Softmax</h4>
            <h4>9.4.3.5 位置编码</h4>
            <h3>9.4.4 为什么有自注意力？</h3>
            <h3>9.4.5 训练</h3>
            <h4>9.4.5.1 训练数据和小批量</h4>
            <h4>9.4.5.2 硬件和日程</h4>
            <h4>9.4.5.3 优化器</h4>
            <h4>9.4.5.4 正则化</h4>

            <h3>9.4.6 结果</h3>
            <h4>9.4.6.1 机器翻译</h4>
            <h4>9.4.6.2 模型变体</h4>
            <h4>9.4.6.3 英语成分分析</h4>
            <h3>9.4.7 总结</h3>
        </main>

            <!-- 右侧小标题导航 -->
            <aside class="col-md-2 right-sidebar">
                <h5>标题</h5>
                <ul id="dynamic-sidebar" class="nav flex-column">
                    <!-- 动态内容 -->
                </ul>
            </aside>
        </div>
    </div>

        <!-- 引入 Bootstrap JS -->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <script>
            // 获取所有的 h3 和 h4 元素
            const sidebar = document.getElementById('dynamic-sidebar');
            const elements = Array.from(document.querySelectorAll('h3, h4')); // 获取所有 h3 和 h4 元素

            // 动态生成目录项
            elements.forEach(element => {
                // 如果没有 id，则为该元素生成一个 id
                if (!element.id) {
                    element.id = `${element.tagName.toLowerCase()}-${Math.random().toString(36).substr(2, 9)}`;
                }

                const link = document.createElement('a');
                link.href = `#${element.id}`;
                link.textContent = element.textContent;

                const listItem = document.createElement('li');
                // 如果是 h4 元素，添加缩进
                if (element.tagName.toLowerCase() === 'h4') {
                    listItem.style.marginLeft = '20px'; // 控制缩进
                }
                listItem.appendChild(link);

                sidebar.appendChild(listItem);
            });

            // 高亮当前目录项
            function highlightCurrentLink(targetId) {
                const links = sidebar.querySelectorAll('a');
                links.forEach(link => {
                    if (link.getAttribute('href') === `#${targetId}`) {
                        link.classList.add('highlight-link');
                    } else {
                        link.classList.remove('highlight-link');
                    }
                });
            }

            // 页面滚动时更新高亮
            window.addEventListener('scroll', () => {
                let found = false;
                elements.forEach(el => {
                    const rect = el.getBoundingClientRect();
                    if (rect.top <= window.innerHeight / 2 && rect.bottom >= 0) {
                        if (!found) {
                            highlightCurrentLink(el.id);
                            found = true;
                        }
                    }
                });
            });

            // 为每个目录链接添加点击事件
            const links = sidebar.querySelectorAll('a');
            links.forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = link.getAttribute('href').substring(1);
                    const targetElement = document.getElementById(targetId);
                    if (targetElement) {
                        targetElement.scrollIntoView({ behavior: 'smooth' });
                        highlightCurrentLink(targetId);
                    }
                });
            });
        </script>
</body>
</html>
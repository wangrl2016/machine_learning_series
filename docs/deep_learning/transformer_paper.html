<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="../laptop_coding_48.png" type="image/png">
    <title>Transformer 论文</title>
    <!-- 引入 Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        .sidebar {
            position: sticky;
            top: 0;
            height: 100vh;
            overflow-y: auto;
            background-color: #f8f9fa;
            color: #000;
            border-right: 1px solid #ddd;
        }

        .custom-title {
            font-size: 2.5rem;
            font-weight: 700;
            color: #4caf50;
            text-align: center;
            margin-bottom: 20px;
            border-bottom: 3px solid #4caf50; /* 标题下边框 */
        }

        .title-subtext {
            font-size: 1.2rem;
            color: #6c757d; /* Bootstrap 灰色 */
            text-align: center;
        }
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background: #f4f4f9;
            color: #333;
        }
        .content {
            padding: 20px;
            background: #fff;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        /* 移动端样式（只在屏幕宽度 < 1024 px 时生效 */
        @media screen and (max-width: 1024px) {
            .sidebar, .right-sidebar {
                display: none; /* 隐藏侧边栏 */
            }
            .content {
                margin: 0;
                padding: 10px;
                width: 100%;
            }
        }
        .content h2 {
            width: 100%;
            font-size: 3rem;
            line-height: 1.6;
            color: #4caf50;
            text-align: center;
            margin-bottom: 40px;
            border-bottom: 3px solid #4caf50;
            display: inline-block;
            padding-bottom: 10px;
            font-weight: bold;
            text-shadow: 1px 1px 5px rgba(0, 0, 0, 0.1);
        }
        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 16px;
            line-height: 1.5;
        }
        .added {
            color: #10e6e2;
            display: inline-block;
            width: 100%;
        }
        code {
            font-family: "Courier New", Courier, monospace;
        }
        .content h3 {
            margin-top: 60px;
            color: #8e44ad;
        }
        .content h4 {
            margin-top: 40px;
            color: #8e44ad;
        }
        .content h5 {
            color: #8e44ad;
        }
        .content p {
            font-size: 1.1em;
            margin: 20px 0;
        }
        .highlight-math {
            background-color: #FFCC99;
            padding: 5px;
            border-radius: 5px;
        }
        .highlight-link {
            color: #f39c12;
            font-weight: bold;
        }
        .highlight {
            background-color: #f1c40f;
            font-weight: bold;
        }
        pre {
            background: #263238;
            color: #c3e88d;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            font-size: 0.9em;
        }
        ul {
            list-style: none;
            padding-left: 20px;
            margin: 10px 0;
        }
        .with-bullets {
            list-style: disc;
            padding-left: 10px;
            margin: 10px 0 10px 20px;
        }
        ul li {
            margin: 4px 0;
            color: #34495e;;
        }
        li .title {
            font-weight: bold;
            color: #2c3e50;
            font-size: 18px;
            margin-bottom: 6px;
        }
        li p {
            color: #34495e;
            font-size: 16px;
        }
        .right-sidebar {
            position: sticky;
            top: 0;
            height: 100vh;
            overflow-y: auto;
            padding: 20px;
            background-color: #f8f9fa;
            border-left: 1px solid #ddd;
        }
        .right-sidebar a {
            color: #343a40;
            text-decoration: none;
        }
        .right-sidebar a:hover {
            text-decoration: underline;
        }
        .right-sidebar .active {
            font-weight: bold;
            color: #007bff;
        }
        #dynamic-sidebar ul {
            list-style-type: none;
            padding-left: 0;
        }
        #dynamic-sidebar ul ul {
            padding-left: 20px;
        }
        a {
            text-decoration: underline;
            text-underline-offset: 4px;
            color: #3498db;
        }
        a:hover {
            color: #2c3e50;
        }
        img {
            display: block;
            margin: 0 auto;
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15), 0 -2px 6px rgba(0, 0, 0, 0.1);;
        }
        .comment {
            color: #6c757d;
            font-size: 0.9em;
            font-style: italic;
            margin: 20px 0;
            border-left: 3px solid #d6d8db;
            padding-left: 10px;
        }
        table {
            width: 50%;
            border-collapse: collapse;
            margin: 20px auto;
            font-family: Arial, sans-serif;
        }
        th, td {
            border: 1px solid #ccc;
            padding: 10px;
            text-align: center;
        }
        th {
            background-color: #f4f4f4;
        }
        .positive {
            color: green;
            font-weight: bold;
        }
        .negative {
            color: red;
            font-weight: bold;
        }
        .scroll-container {
            width: 100%;
            overflow-x: auto;
            overflow-y: hidden;
            -ms-overflow-style: none;
            scrollbar-width: none;
        }
        .paper-comment {
            background-color: #d3d3d3;
            padding: 10px;
            border-radius: 5px;
            font-size: 16px;
            border: 1px solid #a9a9a9;
        }
    </style>
</head>
<body>
    <div class="container-fluid">
        <div class="row">
        <!-- 左侧小节列表 -->
        <nav class="col-md-2 sidebar">

        </nav>

        <!-- 中间内容：显示具体某一个小节 -->
        <main class="col-md-8 content">
            <h2 class="custom-title">9.4 Transformer 论文</h2>
            <p class="title-subtext"><strong style="color: red;">注意力 (Attention)</strong> ，你只需要它！</p>
            <p style="color: #a0a0a0;">创建日期: 2025-01-23</p>
            <p>论文作者：Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin</p>
            <p>主要的 <strong>序列转导模型 (Sequence Transduction Model)</strong> 基于复杂的循环或者卷积神经网络，它们包含一个 <strong>编码器 (Encoder)</strong> 和一个 <strong>解码器 (Decoder)</strong>。性能最佳的模型还通过 <strong>注意力机制 (Attention Mechanism)</strong> 连接编码器和解码器。我们提出一种新的简单的网络架构 <strong style="color: red;">Transformer</strong> ，它只基于注意力机制，完全省去了循环和卷积操作。</p>
            <p>在两个 <em>机器翻译 (Machine Translation)</em> 任务上的实验表明，模型质量优越，并行性更高，并且训练时间明显减少。在 WMT 2014 英语翻译成德语的任务中，我们的模型获得 28.4 BLEU 分数，比现有最佳结果，包括 <em>集成 (Ensemble)</em> 模型，至少提高 2 BLEU 分数。在 WMT 2014 英语翻译成法语的任务中，我们的模型在 8 块 GPU 上训练了 3.5 天，达到了 41.8 BLEU 分数，创新了单模型最好成绩，其训练成本仅为其它最佳模型的一小部分。</p>
            <p class="paper-comment">
                WMT 是 <em>Workshop on Machine Translation</em> 的缩写，WMT 14 指的是 2014 年的机器翻译研讨会，它提供了 <em>英-法</em> 和 <em>英-德</em> 语言数据集。
            </p>
            <p class="paper-comment">
                BLEU 是一个自动评价机器翻译的方法，来自论文 <a href="https://aclanthology.org/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a> ，它通过比较机器翻译结果与人类参考翻译的 N-gram 重叠度来衡量翻译质量，分数越高表示翻译质量越好。
            </p>
            <p>我们通过将 Transformer 成功应用于英语翻译（无论是大量还是有限训练数据的情况下），证明了 Transformer 能很好地泛化到其它任务。</p>

            <p class="comment">8 位作者对论文有同等贡献，名字顺序是随机的，他们每个人所作的工作如下：</p>
            <ul class="with-bullets">
                <li>
                    <p>Jakob 建议用自注意力取代 RNN，并开始努力评估这一想法。</p>
                </li>
                <li>
                    <p>Ashish 和 Illia 设计并实现了第一个 Transformer 模型，并在该工作的方方面面都发挥了至关重要的作用。</p>
                </li>
                <li>
                    <p>Noam 提出了缩放点积注意力机制、多头注意力机制和无参位置表示，也参与了许多细节工作。</p>
                </li>
                <li>
                    <p>Niki 在我们的代码仓和 <a href="https://github.com/google/trax">tensor2tensor</a> （tensor2tensor 已经使用 trax 代替）中设计、实现、调整和评估了无数的模型变体。</p>
                </li>
                <li>
                    <p>Llion 也尝试了模型变体，负责最初的代码库设计以及高效的推理和可视化。</p>
                </li>
                <li>
                    <p>Lukasz 和 Aidan 花了无数个漫长的日子设计并实现了 tensor2tensor 的各个部分，替换了我们之前的代码库，大大改善了结果，并加快了研究。</p>
                </li>
            </ul>
            <h3>9.4.1 介绍</h3>
            <p>循环神经网络，特别是长短期记忆 (LSTM) 和门控循环 (GRU) 神经网络，已经被认为是解决 <em>序列建模 (Sequence Modeling)</em>和转导问题（比如语言建模和机器翻译）的最先进方法。许多研究工作在持续推动循环语言模型和 <em>编码器-解码器 (Encoder-Decoder) 架构的边界。</em></p>
            <p class="paper-comment">LSTM 来自于论文 <a href="https://deeplearning.cs.cmu.edu/S23/document/readings/LSTM.pdf">Long Short-Term Memory</a> ，GRU 来自于论文 <a href="https://arxiv.org/abs/1412.3555">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a> 。</p>
            <p class="comment">语言建模、机器翻译和编码器-解码器相关研究论文：</p>
            <ol>
                <li><p><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1609.08144">Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></p></li>
                <li><p><a href="https://arxiv.org/abs/1602.02410">Exploring the Limits of Language Modeling</a></p></li>
            </ol>
            <p>循环模型通常根据输入和输出序列的符号位置进行计算。通过位置与计算时间步骤对齐，它们会生成一系列的隐状态 \(h_t\) ，它是之前隐状态 \(h_{t-1}\) 和位置 \(t\) 输入的函数。这种固有的顺序性阻碍了训练样本的并行化，这在较长的序列中尤其重要，因为内存限制了样本之间的批处理。</p>
            <p>最近的研究通过 <em>分解技巧 (Factoization Trick)</em> 和 <em>条件计算 (Conditional Computation)</em> ，显著提高了计算效率，同时后者也能提高模型性能。然而，顺序计算的基本约束仍然存在。</p>
            <p class="paper-comment">分解技巧来自于论文 <a href="https://arxiv.org/abs/1703.10722">Factorization tricks for LSTM networks</a> ，条件计算来自于论文 <a href="https://arxiv.org/abs/1701.06538">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a> 。</p>
            <p>注意力机制已经称为序列建模和转导模型不可或缺的一部分，它允许对依赖关系进行建模，而无需考虑它们在输入或输出序列中的距离。然而，除了少数情况外，这种注意力机制都是与循环网络结合使用的。</p>
            <p class="paper-comment">论文 <a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a> 和 <a href="https://arxiv.org/abs/1702.00887">Structured Attention Networks</a> 使用注意力机制，论文 <a href="https://arxiv.org/abs/1606.01933">A Decomposable Attention Model for Natural Language Inference</a> 使用注意力将问题分解成可以解决的子问题，实现并行化。</p>
            <p>在这项工作中，我们提出了 Transformer ，这是一种避免循环、完全依靠注意力机制来绘制输入和输出之间的全局依赖关系的架构。Transformer 显著提高了并行化水平，在 8 个 P100 GPU 上训练 12 小时后，就可以达到最新的翻译质量水平。</p>

            <h3>9.4.2 背景</h3>
            <p>减少序列计算的目标也是 <em>Extended Neural GPU</em> 、<em>ByteNet</em> 和 <em>ConvS2S</em> 的基础，它们都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的 <em>隐藏表示 (Hidden Representation)</em> 。在这些模型中，关联两个任意输入或输出位置的信号所需的操作数随着位置之间的距离而增加，对于 <em>ConvS2s</em> 呈线性增长，对于 <em>ByteNet</em> 呈对数增长。这使得学习远距离位置之间的依赖关系变得更加困难。</p>
            <p>在 Transformer 中，这被减少到一个恒定数量的操作，尽管由于平均注意力加权位置而降低了有效分辨率，但我们使用 <a href="#attention">第 9.4.3.2 小节 注意力</a> 中描述的多头注意力来抵消这种影响。</p>
            <p><strong>自注意力 (Self-attention)</strong> 有时也称为内部注意力，是一种将单个序列的不同位置关联起来以计算该序列的表示的注意力机制。自注意力已成功用于各种任务，包括阅读裂解、抽象总结、语言推理和句子表示。</p>
            <p>基于循环注意力机制，而非序列对齐循环的端到端记忆网络，已在简单语言问答和语言建模任务上取得良好表现。</p>
            <p>然后，据我们所知，Transformer 是第一个完全依赖自注意力来计算其输入和输出表示的传导模型，而无需使用序列对齐的 RNN 或者 CNN 。在以下部分中，我们将描述 Transformer ，为什么使用自注意力，讨论相对于其它模型的优点。</p>

            <h3>9.4.3 模型架构</h3>
            <p>大多数具有竞争力的神经序列转导模型都具有编码器-解码器结构。这里，编码器将符号表示的输入序列 \((x_1, ... , x_n)\) 映射成连续表示的序列 \(z = (z_1, ... , z_n)\) 。给定 \(z\) ，解码器生成一个输出符号序列 \((y_1, ... , y_m)\) ，每次生成一个元素。在每个步骤中，模型都是自回归的，在生成下一个符号时将先前生成的符号作为附加输入。</p>
            <img src="res/transformer.png" alt="Transformer 架构" width="400">
            <p>Transformer 整体架构如上图所示，编码器（左边）和解码器（右边）都使用多头自注意力和全连接层。</p>
            <h4>9.4.3.1 编码器和解码器栈</h4>
            <p><strong>编码器 (Encoder)</strong> ：编码器由 \(N = 6\) 个相同层堆叠而成。每个层有两个子层，第一个子层是多头自注意力机制，第二个是简单的全连接前馈网络。两个子层都分别采用了残差连接，然后进行了层归一化。</p>
            <p>也就是说，每个子层的输出为 <em>LayerNorm(x + Sublayer(x))</em> ，其中 <em>Sublayer(x)</em> 是子层本身实现的函数。为了促进这些残差连接，模型中的所有子层，包括嵌入层，产生的输出维度都是 \(d_{model} = 512\) 。</p>
            <p><strong>解码器 (Decoder)</strong> ：解码器也由 \(N = 6\) 个相同的层堆叠而成，除了编码器的两个子层外，解码器还插入第三个子层，该子层对编码器的输出执行多头注意力。与编码器类似，我们在每个子层中采用残差连接，然后进行层归一化。</p>
            <p>我们还修改了解码器堆栈中的自注意力子层，以防止在当前位置时关注后续位置。这种掩盖，加上输出偏移一个位置的事实，确保位置 \(i\) 的预测智能依赖于小于 \(i\) 的位置的已知输出。</p>
            
            <h4 id="attention">9.4.3.2 注意力</h4>
            <p>一个注意力函数可以描述为将 <strong>查询 (Query)</strong> 和一组 <strong>键-值 (Key-Value)</strong> 对映射到输出，其中查询、键、值和输出都是向量。输出被计算成为值得加权和，其中分配给每个值得权重由查询与相应键得兼容性函数计算得出。</p>

            <h5>缩放点积注意力</h5>
            <p>我们称此论文的注意力为 <strong>缩放点积注意力 (Scale Dot-Product Attention)</strong> 。查询和键的维度是 \(d_k\) ，值的维度是 \(d_v\) ，它们组成输入。我们计算查询和所有键的点积，除以 \(\sqrt{d_k}\) ，使用 softmax 函数获取在值上的权重。</p>
            <p>在实践中，我们同时在一系列的查询中计算注意力函数，将它合并成矩阵 \(Q\) ，键和值也分别合并成矩阵 \(K\) 和 \(V\) 。使用如下公式计算输出矩阵：</p>
            <div class="scroll-container highlight-math" style="text-align: center;">
                \(Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\)
            </div>
            <p>最常用的两个注意力函数是加性注意力和点积注意力，点积注意力和我们的算法相似，除了缩放因子 \(\frac{1}{\sqrt{d_k}}\) 。加性注意力使用具有单个隐藏层的前馈网络来进行兼容性计算。虽然两者在理论上复杂度相似，但点积注意力在实践中更快、更节省空间，因为它可以使用高度优化的矩阵乘法来实现。</p>
            <p>虽然对于较小的 \(d_k\) 值，两种机制得表现相似。但对于较大的 \(d_k\) 值，加法注意力优于点积注意力（没有除以 \(\sqrt{d_k}\)）。我们怀疑，对于较大的 \(d_k\) 值，点积的值会变大，从而将 softmax 推入具有极小梯度的区域。为了抵消这种影响，我们将点积乘以 \(\frac{1}{\sqrt{d_k}}\) 。</p>
            <p class="comment">为了说明点积为何变大，假设 \(q\) 和 \(k\) 是独立的随机变量，它们的均值为 0 ，方差为 1 ，那么它们的点积 \(q \cdot k = \sum_{i=1}^{d_k}q_{i}k_i\) ，它的均值为 0 ，方差为 \(d_k\) 。</p>

            <h5>多头注意力</h5>
            <p>与其在维度为 \(d_{model}\) 的 键、值和查询中执行单个注意力函数，我们发现将查询、键和值使用可学习参数将它们线性投影到 \(d_k\) 、\(d_k\) 和 \(k_v\) 维，重复上述操作 \(h\) 次，这样效果更好。然后在每个投影的查询、键和值上，并行执行注意力函数，得到 \(d_v\) 维度的输出值。这些值连接起来，再次投影，得到最终值。</p>
            <p>多头注意力机制允许模型联合起来关注不同位置的不同表示子空间的信息。使用单个注意力头时，平均化会抑制这一点。</p>
            <div class="scroll-container highlight-math" style="text-align: center;">
                \(MultiHead(Q, K, V) = Concat(head_1, ... , head_h)W^O\) <br> \(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\) 
            </div>
            <p>其中投影是参数矩阵 \(W_i^Q \in R^{d_{model} \times d_k}\) ， \(W_i^K \in R^{d_{model} \times d_k}\)  ， \(W_i^V \in R^{d_{model} \times d_v}\) 和 \(W^O \in R^{hd_v} \times d_{model}\) 。</p>
            <p>在本工作中，我们采用了 \(h = 8\) 个并行注意力层，对于每个层，我们使用 \(d_k = d_v = d_{model} / h = 64\) ，由于每层的维度减小，总计算成本与具有全维度的单头注意力相似。</p>

            <h5>注意力在模型的应用</h5>
            <p>Transformer 通过三种不同的方式使用多头注意力机制：</p>
            <ul class="with-bullets">
                <li><p>在编码器-解码器注意力层，查询来自解码器，键和值来自编码器的输出。这允许解码器中的每个位置关注输入序列中的所有位置。模仿了序列到序列模型中的典型编码器-解码器注意机制。</p></li>
                <li><p>编码器包含自注意力层，在自注意力层中，所有的键、值和查询都来自相同的地方，也就是说，编码器中的前一层输出。编码器中的每个位置都可以关注前一层的所有位置。</p></li>
                <li><p>类似地，解码器中的自注意力层允许解码器中的每个位置关注当前位置之前的所有位置。我们需要防止解码器看到当前位置之后的位置，以保留自回归属性。我们在缩放点积注意力中实现了这一点，方式是将 softmax 中的非法输入设置为 \(-\infty\) 。</p></li>
            </ul>

            <h4>9.4.3.3 位置前馈网络</h4>
            <p>除了注意力子层之外，我们的编码器和解码器中的每一层都包含一个完全连接的前馈网络，该网络分别且相同地应用于每个位置。它由两个线性变换和其间的 ReLU 激活组成。</p>
            <div class="scroll-container highlight-math" style="text-align: center;">
                \(FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\) 
            </div>
            <p>虽然线性变换在不同位置上是相同的，但它们在层与层之间使用不同的参数。另一种描述方式是将其作为两个卷积，内核大小为 1 。输入和输出的维度 \(d_{model} = 512\) ，内部隐藏层的维度 \(d_{ff} = 2048\) 。</p>
            <h4>9.4.3.4 嵌入和 Softmax</h4>
            <p>和其它序列转导模型类似，我们使用嵌入层将输入和输出标记转换为维度为 \(d_{model}\) 的向量。我们还使用线性变换和 softmax 函数将解码器输出转换为预测下一个标记的概率。在我们的模型中，两个嵌入层和 softmax 线性变换使用相同的矩阵。在嵌入层，我们将权重乘以 \(\sqrt_{model}\) 。</p>

            <h4>9.4.3.5 位置编码</h4>
            <p>因为我们的模型不包含循环和卷积操作，为了让模型利用序列的顺序信息，我们必须注入以下有个序列中标记的相对或绝对位置的信息。</p>
            <p>为此，我们在编码器和解码器堆栈底部的输入嵌入中添加了位置编码，位置编码具有与嵌入相同的维度 \(d_{model}\) ，因此它们可以直接相加，位置编码有很多种选择。</p>

            <p>在本工作中，我们使用不同频率的正弦和余弦函数：</p>
            <div class="scroll-container highlight-math" style="text-align: center;">
                \(PE_{(pos, 2i)} = sin(pos / 10000^{2i/d_{model}})\) 
            </div>
            <div class="scroll-container highlight-math" style="text-align: center;">
                \(PE_{(pos, 2i+1)} = cos(pos / 10000^{2i/d_{model}})\) 
            </div>
            <p>其中 \(pos\) 是位置，\(i\) 是维度。也就是说，位置编码的每个维度都对应一个正弦曲线。波长从 \(2\pi\) 到 \(10000 \dot 2\pi\) 呈几何级数。</p>
            <p>我们之所以选择这个函数，是因为我们假设它可以允许模型轻松地通过相对位置来学习关注，因为对于任何固定偏移量 \(k\) ，\(PE_{pos+k}\) 可以被表示成 \(PE_{pos}\) 的线性函数。</p>
            <p>我们还尝试了可学习的位置嵌入，发现两个版本产生的结果几乎相同。选择正弦版本，是因为它可能允许模型推断出比训练期间遇到的序列长度更长的序列长度。</p>

            <h3>9.4.4 自注意力？</h3>
            <h3>9.4.5 训练</h3>
            <h4>9.4.5.1 训练数据和小批量</h4>
            <h4>9.4.5.2 硬件和日程</h4>
            <h4>9.4.5.3 优化器</h4>
            <h4>9.4.5.4 正则化</h4>

            <h3>9.4.6 结果</h3>
            <h4>9.4.6.1 机器翻译</h4>
            <h4>9.4.6.2 模型变体</h4>
            <h4>9.4.6.3 英语成分分析</h4>
            <h3>9.4.7 总结</h3>
        </main>

            <!-- 右侧小标题导航 -->
            <aside class="col-md-2 right-sidebar">
                <h5><a href="index.html#section9">Transformer 架构</a></h5>
                <ul id="dynamic-sidebar" class="nav flex-column">
                    <!-- 动态内容 -->
                </ul>
            </aside>
        </div>
    </div>

        <!-- 引入 Bootstrap JS -->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <script>
            // 获取所有的 h3 和 h4 元素
            const sidebar = document.getElementById('dynamic-sidebar');
            const elements = Array.from(document.querySelectorAll('h3, h4')); // 获取所有 h3 和 h4 元素

            // 动态生成目录项
            elements.forEach(element => {
                // 如果没有 id，则为该元素生成一个 id
                if (!element.id) {
                    element.id = `${element.tagName.toLowerCase()}-${Math.random().toString(36).substr(2, 9)}`;
                }

                const link = document.createElement('a');
                link.href = `#${element.id}`;
                link.textContent = element.textContent;

                const listItem = document.createElement('li');
                // 如果是 h4 元素，添加缩进
                if (element.tagName.toLowerCase() === 'h4') {
                    listItem.style.marginLeft = '20px'; // 控制缩进
                }
                listItem.appendChild(link);

                sidebar.appendChild(listItem);
            });

            // 高亮当前目录项
            function highlightCurrentLink(targetId) {
                const links = sidebar.querySelectorAll('a');
                links.forEach(link => {
                    if (link.getAttribute('href') === `#${targetId}`) {
                        link.classList.add('highlight-link');
                    } else {
                        link.classList.remove('highlight-link');
                    }
                });
            }

            // 页面滚动时更新高亮
            window.addEventListener('scroll', () => {
                let found = false;
                elements.forEach(el => {
                    const rect = el.getBoundingClientRect();
                    if (rect.top <= window.innerHeight / 2 && rect.bottom >= 0) {
                        if (!found) {
                            highlightCurrentLink(el.id);
                            found = true;
                        }
                    }
                });
            });

            // 为每个目录链接添加点击事件
            const links = sidebar.querySelectorAll('a');
            links.forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = link.getAttribute('href').substring(1);
                    const targetElement = document.getElementById(targetId);
                    if (targetElement) {
                        targetElement.scrollIntoView({ behavior: 'smooth' });
                        highlightCurrentLink(targetId);
                    }
                });
            });
        </script>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="../laptop_coding_48.png" type="image/png">
    <title>Transformer 翻译模型</title>
    <!-- 引入 Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="../article_style.css" rel="stylesheet">
</head>
<body>
    <div class="container-fluid">
        <div class="row">
        <!-- 左侧小节列表 -->
        <nav class="col-md-2 sidebar">

        </nav>

        <!-- 中间内容：显示具体某一个小节 -->
        <main class="col-md-8 content">
            <h2 class="custom-title">7.4 Transformer 翻译模型</h2>
            <p class="title-subtext">使用 Transformer 和 Keras 进行神经机器翻译！</p>
            <p style="color: #a0a0a0;">创建日期: 2025-04-15</p>

            <p>本教程演示如何创建和训练 <em>序列到序列 (Seq2Seq) </em>的 Transformer 模型，它将葡萄牙语翻译成英语。Transformer 最初在论文 Attention is all you need 中提出，可以查看《深度学习综合指南》<a href="../deep_learning/transformer_paper.html">第 9.4 节 Transformer 论文</a> 的内容。</p>
            <p>Transformer 是一种深度神经网络，它利用 <em>自注意力机制 (Self-attention)</em> 取代了 CNN 和 RNN 。自注意力机制让 Transformer 能够轻松地在输入序列之间传递信息。</p>
            <p>正如 <a href="https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/">Google AI 博客文章</a> 中所解释的那样：</p>

            <p><em>机器翻译的神经网络通常包含一个编码器，用于读取输入句子并生成其表示。然后，解码器参考编码器生成的表示，逐字生成输出句子。Transformer 首先为每个单词生成初始表示或嵌入...然后它使用自注意力机制聚合来自所有其它单词的信息，根据整个上下文为每个单词生成一个新的表示，由下图填充的球表示。这个过程对所有单词并行重复多次，依次生成新的表示：</em></p>
            <img src="res/transform_20fps.gif" alt="transform 示意图" width="600"> 

            <p>需要消化的内容很多，本教程的目标是将其分解为易于理解的部分。在本教程中，我们将：</p>
            <ul class="with-bullets">
                <li>
                    <p>准备数据；</p>
                </li>
                <li>
                    <p>实现必要的组件：</p>
                    <ul class="with-bullets">
                        <li>位置嵌入；</li>
                        <li>注意力层；</li>
                        <li>编码器和解码器。</li>
                    </ul>
                </li>
                <li>
                    <p>构建和训练 Transformer ；</p>
                </li>
                <li>
                    <p>生成翻译；</p>
                </li>
                <li>
                    <p>导出模型。</p>
                </li>
            </ul>

            <p>为了充分利用本教程，如果您了解 <a href="character_based_rnn.html">第 7.2 节 文本生成</a> 和 <a href="../deep_learning/attention_mechanism.html">注意力机制</a> 的知识，这将很有帮助。</p>

            <p>Transformer 是一种序列到序列的编码器-解码器模型，类似于 <a href="translate_seq2seq.html">第 7.3 节 Seq2Seq 翻译模型</a> 中的模型。单层 Transformer 需要编写更多代码，但与编码器-解码器 RNN 模型几乎相同，如下图所示展示带有注意力的 RNN 模型：</p>
            <img src="res/rnn_attention_words.png" alt="RNN 注意力" width="400">
            <p>唯一的区别是 RNN 层被替换为自注意力层。本教程构建了一个 4 层 Transformer，它更强大，但从根本上来说并不更复杂。如下图展示单层 Transformer 模型：</p>
            <img src="res/transformer_1layer_words.png" alt="Transformer 1层" width="400">
            <p>训练完模型后，我们可以输入葡萄牙语句子并返回英语翻译。可以看到生成的可视化注意力权重：</p>

            <h3>7.4.1 为何如此重要</h3>

            <ul class="with-bullets">
                <li>
                    <p>Transformer 擅长对序列数据（例如自然语言）进行建模。</p>
                </li>
                <li>
                    <p>与循环神经网络 (RNN) 不同，Transformer 是可并行化的。这使得他们在 GPU 和 TPU 等硬件上非常高效。主要原因是 Transformer 用注意力机制取代了循环，计算可以并行，而不像 RNN 那样串联计算。</p>
                </li>
                <li>
                    <p>与 RNN（如 Seq2Seq）或卷积神经网络 (CNN) 不同，Transformer 能够捕获输入或输出序列中远距离位置之间的上下文和依赖。因此，长连接可以被学习。在每一层中注意力机制允许每个位置访问整个输入，而 RNN 或者 CNN ，信息需要经过许多处理步骤才能移动很长距离，这使得学习变得困难。</p>
                </li>
                <li>
                    <p>Transformer 不对数据中的时空关系做任何假设，这对于处理一组对象非常理想。</p>
                </li>
            </ul>
            <p>下图展示在英语到法语上训练的Transformer 的第 5 层和第 6 层，单词 "it" 的编码器自注意力分布（八个注意力头之一）：</p>
            <img src="res/self_attention_distrib.png" alt="自注意力分布" width="600">

            <h3>7.4.2 设置</h3>
            <p>需要安装 <a>TensorFlow Datasets</a> 下载数据集，和 <a>TensroFlow Text</a> 进行文本预处理，导入必要的模块：</p>
            <pre><code>from matplotlib import pyplot
import tensorflow as tf
import tensorflow_text
import tensorflow_datasets as tfds
import keras
import numpy</code></pre>

            <h3>7.4.3 数据处理</h3>
            <p>下载数据集和字词标记器，将它们生成一个 <code>tf.data.Dataset</code> 结构体。</p>

            <h4>7.4.3.1 下载数据集</h4>
            <p>使用 <em>TensorFlow Datasets</em> 加载 <a href="https://www.tensorflow.org/datasets/catalog/ted_hrlr_translate#ted_hrlr_translatept_to_en">葡萄牙语-英语翻译数据集</a>，这个数据集包含接近 52000 个训练，1200 验证和 1800 个测试样本：</p>
            <pre><code>examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',
            with_info=True,
            as_supervised=True)
train_examples, val_examples = examples['train'], examples['validation']</code></pre>
            <p><em>TensorFlow Datasets</em> 返回 <code>tf.data.Dataset</code> 对象，它可以产生成对的文本示例 (葡萄牙语-英语) ：</p>
            <pre><code>for pt_examples, en_examples in train_examples.batch(3).take(1):
    print('> Examples in Portuguese:')
    for pt in pt_examples.numpy():
        print(pt.decode('utf-8'))
    print('> Examples in English:')
    for en in en_examples.numpy():
        print(en.decode('utf-8'))</code></pre>
            <pre><samp>&gt; Examples in Portuguese:
e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .
mas e se estes fatores fossem ativos ?
mas eles não tinham a curiosidade de me testar .
&gt; Examples in English:
and when you improve searchability , you actually take away the one advantage of print , which is serendipity .
but what if it were active ?
but they did n't test for curiosity .</samp></pre>

            <h4>7.4.3.2 设置标记器</h4>
            <p>我们已经加载了数据集，接下来需要将文本 <strong>标记 (Tokenize)</strong> ，这样每个元素可以使用 <em>标记 (Token)</em> 或者标记 ID（数字）表示。</p>
            <p>标记化是一个将文本分解成标记的过程。取决于标记器，这些标记可以表示句子片段、单词、子单词或者字符。要了解更多标记器的知识，可以访问 <a href="https://www.tensorflow.org/text/guide/tokenizers">Tokenizing with TF Text</a> 文档。</p>
            <p>本教程使用的是 <a href="https://www.tensorflow.org/text/guide/subwords_tokenizer">Subword tokenizers</a> 中的内置标记器。它优化了两个 <code>text.BertTokenizer</code> 对象（一个是英语，一个是葡萄牙语），使用 TensorFlow 的 <code>save_model</code> 格式导出。</p>
            <p class="comment">与 <a href="https://arxiv.org/abs/1706.03762">原始论文</a> 5.1 小节 不同，它们对源句子和目标句子使用单 <em>字节对 (Byte-pair)</em> 标记器，总词汇量为 37000 。</p>
            <p>下载、解压、导入 <code>save_model</code> 格式：</p>
            <pre><code>model_name = 'ted_hrlr_translate_pt_en_converter'
keras.utils.get_file(
    f'{model_name}.zip',
    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',
    cache_dir='.', cache_subdir='', extract=True)
tokenizers = tf.saved_model.load('ted_hrlr_translate_pt_en_converter_extracted/' + model_name)</code></pre>
            <p><code>tf.saved_model</code> 包含两个文本标记器，一个是英语，一个是葡萄牙语，它们都有相同的方法。</p>
            <p><code>tokenize</code> 方法将一批字符串转换为填充的标记 ID ，此方法在标记之前拆分标点符号、小写字母并对输入进行 Unicode 规范化。该标准化在此处不可见，因为输入数据已经标准化：</p>
            <pre><code>print('> This is a batch of strings:')
for en in en_examples.numpy():
    print(en.decode('utf-8'))</code></pre>
            <pre><code>encoded = tokenizers.en.tokenize(en_examples)
print('> This is a padded-batch of token IDs:')
for row in encoded.to_list():
    print(row)</code></pre>
            <pre><samp>&gt; This is a padded-batch of token IDs:
[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]
[2, 87, 90, 107, 76, 129, 1852, 30, 3]
[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]</samp></pre>
            
            <p><code>detokenize</code> 方法尝试将这些标记 ID 转换回人类可读的文本：</p>
            <pre><code>round_trip = tokenizers.en.detokenize(encoded)
print('> This is human-readable text:')
for line in round_trip.numpy():
    print(line.decode('utf-8'))</code></pre>

            <p>底层 <code>lookup</code> 方法将标记 ID 转换为标记文本：</p>
            <pre><code>print('> This is the text split into tokens:')
tokens = tokenizers.en.lookup(encoded)
print(tokens)</code></pre>
            <p>输出展示了 <em>子词 (Subword)</em> 标记器可以对单子进行分割：</p>
            <pre><samp>&gt; This is the text split into tokens:
&lt;tf.RaggedTensor [[b'[START]', b'and', b'when', b'you', b'improve', b'search', b'##ability',
     b',', b'you', b'actually', b'take', b'away', b'the', b'one', b'advantage',
     b'of', b'print', b',', b'which', b'is', b's', b'##ere', b'##nd', b'##ip',
     b'##ity', b'.', b'[END]']                                                 ,
    [b'[START]', b'but', b'what', b'if', b'it', b'were', b'active', b'?',
     b'[END]']                                                           ,
    [b'[START]', b'but', b'they', b'did', b'n', b"'", b't', b'test', b'for',
     b'curiosity', b'.', b'[END]'] ]&gt;</samp></pre>
            <p>比如单词 <em>searchability</em> 分解成为 <em>search</em> 和 <em>##ability</em> ，单词 <em>serendipity</em> 分解成 <em>s</em> , <em>##ere</em> , <em>##nd</em> , <em>##ip</em> 和 <em>##ity</em> 。</p>
            <p>需要注意的是标记的文本包含 <em>[START]</em> 和 <em>[END]</em> 两个标记。</p>
            
            <h4>7.4.3.3 使用 tf.data</h4>

            <h3>7.4.4 测试数据集</h3>

            <h3>7.4.5 定义组件</h3>

            <p>Transformer 内部有很多事情要做。需要记住的重要事项是：</p>
            <ol>
                <li>
                    <p>它遵循与带有编码器和解码器的标准序列到序列模型相同的一般模式；</p>
                </li>
                <li>
                    <p>如果我们一步一步地努力，一切都会变得有意义。</p>
                </li>
            </ol>

            <img src="res/transformer.png" alt="原始 Transformer 架构" width="400">

            <p>随着学习本教程的进度，我们将解释这两个图中的每个组件。下图是 4 层的 Transformer 表示：</p>
            <img src="res/transformer_4layer.png" alt="4 层 Transformer" width="400">

            <h4>7.4.5.1 嵌入和位置编码</h4>

            <p>编码器和解码器的输入使用相同的嵌入和位置编码逻辑。</p>
            <p>给定一个标记序列，输入标记（葡萄牙语）和目标标记（英语）都必须使用一个层转换为 <code>keras.layers.Embedding</code> 向量。</p>
            <p>整个模型中使用的注意力层将其输入市委一组无序的向量。由于模型不包含任何循环层或者卷积层。它需要某种方法来识别词序，否则它会将输入序列市委一个词语背包，例如 <em>how are you</em> 等它是无法区分 <em>how you are</em> 和 <em>you how are</em> 的，</p>
            <p>Transformer 为嵌入向量添加了位置编码。它使用一组不同频率的正弦和余弦（跨序列）。根据定义，附近的元素将具有相似的位置编码。</p>
            <p>原始论文采用以下公式来计算位置编码：</p>

            <p>位置编码函数是一堆正弦和余弦，它们根据沿嵌入向量深度的位置以不同的频率振动，</p>

            <h4>7.4.5.2 添加并规范化</h4>
            <h4>7.4.5.3 基础注意力层</h4>

            <p>整个模型都使用了注意力层，除了注意力的配置方式外，这些层都是相同的。每个层都包含一个 <code>layers.MultiHeadAttention</code> 、一个 <code>layers.LayerNormalization</code> 和一个 <code>layers.Add</code> 。</p>

            <p>要实现这些注意层，请从仅包含组件层的简单基类开始。每个用例都将作为子类实现。这样编写的代码会多一点，但意图却很明确。</p>
            <h4>7.4.5.4 交叉注意力层</h4>
            <h4>7.4.5.5 全局自注意力层</h4>
            <h4>7.4.5.6 因果自注意力层</h4>
            <h4>7.4.5.7 前馈网络</h4>
            <h4>7.4.5.8 编码器层</h4>
            <h4>7.4.5.9 编码器</h4>
            <h4>7.4.5.10 解码器层</h4>
            <h4>7.4.5.11 解码器</h4>
            
            <h3>7.4.6 Transformer</h3>

            <h3>7.4.7 训练</h3>

            <h3>7.4.8 运行推理</h3>

            <h3>7.4.9 创建注意力图</h3>

            <h3>7.4.10 导出模型</h3>

            <h3>7.4.11 结论</h3>

            <div class="navigation">
                <a href="translate_seq2seq.html">上一节：7.3 Seq2Seq 翻译模型</a>
                <a href="#">下一节：7.5 </a>
            </div>
        </main>

            <!-- 右侧小标题导航 -->
            <aside class="col-md-2 right-sidebar">
                <h5><a href="index.html#section7">文本</a></h5>
                <ul id="dynamic-sidebar" class="nav flex-column">
                    <!-- 动态内容 -->
                </ul>
            </aside>
        </div>
    </div>

        <!-- 引入 Bootstrap JS -->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <script>
            // 获取所有的 h3 和 h4 元素
            const sidebar = document.getElementById('dynamic-sidebar');
            const elements = Array.from(document.querySelectorAll('h3, h4')); // 获取所有 h3 和 h4 元素

            // 动态生成目录项
            elements.forEach(element => {
                // 如果没有 id，则为该元素生成一个 id
                if (!element.id) {
                    element.id = `${element.tagName.toLowerCase()}-${Math.random().toString(36).substr(2, 9)}`;
                }

                const link = document.createElement('a');
                link.href = `#${element.id}`;
                link.textContent = element.textContent;

                const listItem = document.createElement('li');
                // 如果是 h4 元素，添加缩进
                if (element.tagName.toLowerCase() === 'h4') {
                    listItem.style.marginLeft = '20px'; // 控制缩进
                }
                listItem.appendChild(link);

                sidebar.appendChild(listItem);
            });

            // 高亮当前目录项
            function highlightCurrentLink(targetId) {
                const links = sidebar.querySelectorAll('a');
                links.forEach(link => {
                    if (link.getAttribute('href') === `#${targetId}`) {
                        link.classList.add('highlight-link');
                    } else {
                        link.classList.remove('highlight-link');
                    }
                });
            }

            // 页面滚动时更新高亮
            window.addEventListener('scroll', () => {
                let found = false;
                elements.forEach(el => {
                    const rect = el.getBoundingClientRect();
                    if (rect.top <= window.innerHeight / 2 && rect.bottom >= 0) {
                        if (!found) {
                            highlightCurrentLink(el.id);
                            found = true;
                        }
                    }
                });
            });

            // 为每个目录链接添加点击事件
            const links = sidebar.querySelectorAll('a');
            links.forEach(link => {
                link.addEventListener('click', (e) => {
                    e.preventDefault();
                    const targetId = link.getAttribute('href').substring(1);
                    const targetElement = document.getElementById(targetId);
                    if (targetElement) {
                        targetElement.scrollIntoView({ behavior: 'smooth' });
                        highlightCurrentLink(targetId);
                    }
                });
            });
        </script>
</body>
</html>
